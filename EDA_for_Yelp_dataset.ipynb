{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA for Yelp Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========Data Wrangling===========\n",
    "import os\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# =======For Preprocessing=======\n",
    "import xlrd\n",
    "import string\n",
    "import re\n",
    "import langid\n",
    "from transformers import *\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# ==========For Model===========\n",
    "from transformers import BertForSequenceClassification # Model\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup # Optimizer\n",
    "from sklearn.metrics import f1_score  # Metrics\n",
    "\n",
    "\n",
    "# ===========PYTORCH===========\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# ============NLTK============\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from nltk.stem.porter import *\n",
    "from nltk.probability import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_pd = pd.read_excel(\"../data_set/Yelp Labelled Review Dataset with Sentiments and Features.xlsx1\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_id</th>\n",
       "      <th>Product_id</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Date</th>\n",
       "      <th>Review</th>\n",
       "      <th>Spam(1) and Not Spam(0)</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>923</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2014-01-30</td>\n",
       "      <td>The food at snack is a selection of popular Gr...</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "      <td>['appetizer tray', 'greek salad', 'main courses']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>924</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2011-05-05</td>\n",
       "      <td>This little place in Soho is wonderful. I had ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "      <td>['little place', 'soho', 'lamb sandwich', 'soh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>925</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2011-12-30</td>\n",
       "      <td>ordered lunch for 15 from Snack last Friday. Ã...</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "      <td>['snack', 'regular company lunch list']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>926</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2012-10-04</td>\n",
       "      <td>This is a beautiful quaint little restaurant o...</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "      <td>['beautiful quaint', 'pretty street', 'great p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>927</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2014-02-06</td>\n",
       "      <td>Snack is great place for a Ã‚Â casual sit down...</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "      <td>['snack', 'great place', 'Ã¢ casual', 'cold wi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_id  Product_id  Rating       Date  \\\n",
       "0      923           0       3 2014-01-30   \n",
       "1      924           0       3 2011-05-05   \n",
       "2      925           0       4 2011-12-30   \n",
       "3      926           0       4 2012-10-04   \n",
       "4      927           0       4 2014-02-06   \n",
       "\n",
       "                                              Review  Spam(1) and Not Spam(0)  \\\n",
       "0  The food at snack is a selection of popular Gr...                        1   \n",
       "1  This little place in Soho is wonderful. I had ...                        1   \n",
       "2  ordered lunch for 15 from Snack last Friday. Ã...                        1   \n",
       "3  This is a beautiful quaint little restaurant o...                        1   \n",
       "4  Snack is great place for a Ã‚Â casual sit down...                        1   \n",
       "\n",
       "  Sentiment                                           Features  \n",
       "0  Positive  ['appetizer tray', 'greek salad', 'main courses']  \n",
       "1  Positive  ['little place', 'soho', 'lamb sandwich', 'soh...  \n",
       "2  Positive            ['snack', 'regular company lunch list']  \n",
       "3  Positive  ['beautiful quaint', 'pretty street', 'great p...  \n",
       "4  Positive  ['snack', 'great place', 'Ã¢ casual', 'cold wi...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_id</th>\n",
       "      <th>Product_id</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Date</th>\n",
       "      <th>Review</th>\n",
       "      <th>Spam(1) and Not Spam(0)</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>355205</th>\n",
       "      <td>161146</td>\n",
       "      <td>349</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-10-04</td>\n",
       "      <td>The aircondition makes so much noise and its ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355206</th>\n",
       "      <td>116424</td>\n",
       "      <td>349</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-05-27</td>\n",
       "      <td>Even though the pictures show very clean room...</td>\n",
       "      <td>0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>['clean rooms', 'actual room', 'o clock']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355207</th>\n",
       "      <td>161147</td>\n",
       "      <td>349</td>\n",
       "      <td>2</td>\n",
       "      <td>2011-03-03</td>\n",
       "      <td>Backyard of the hotel is total mess shouldn t...</td>\n",
       "      <td>0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>['backyard', 'total mess shouldn t']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355208</th>\n",
       "      <td>97930</td>\n",
       "      <td>349</td>\n",
       "      <td>2</td>\n",
       "      <td>2014-07-29</td>\n",
       "      <td>You When I booked with your company on line y...</td>\n",
       "      <td>0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>['s room', 'villa suite theough', 'wife s 40th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355209</th>\n",
       "      <td>5260</td>\n",
       "      <td>349</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-02-07</td>\n",
       "      <td>My room was dirty and I was afraid to walk ba...</td>\n",
       "      <td>0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>['white furniture', 'angry dog', 'shower drain...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        User_id  Product_id  Rating       Date  \\\n",
       "355205   161146         349       1 2012-10-04   \n",
       "355206   116424         349       1 2013-05-27   \n",
       "355207   161147         349       2 2011-03-03   \n",
       "355208    97930         349       2 2014-07-29   \n",
       "355209     5260         349       1 2013-02-07   \n",
       "\n",
       "                                                   Review  \\\n",
       "355205   The aircondition makes so much noise and its ...   \n",
       "355206   Even though the pictures show very clean room...   \n",
       "355207   Backyard of the hotel is total mess shouldn t...   \n",
       "355208   You When I booked with your company on line y...   \n",
       "355209   My room was dirty and I was afraid to walk ba...   \n",
       "\n",
       "        Spam(1) and Not Spam(0) Sentiment  \\\n",
       "355205                        0  Negative   \n",
       "355206                        0  Negative   \n",
       "355207                        0  Negative   \n",
       "355208                        0  Negative   \n",
       "355209                        0  Negative   \n",
       "\n",
       "                                                 Features  \n",
       "355205                                                 []  \n",
       "355206          ['clean rooms', 'actual room', 'o clock']  \n",
       "355207               ['backyard', 'total mess shouldn t']  \n",
       "355208  ['s room', 'villa suite theough', 'wife s 40th...  \n",
       "355209  ['white furniture', 'angry dog', 'shower drain...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_pd.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 355210 entries, 0 to 355209\n",
      "Data columns (total 8 columns):\n",
      " #   Column                   Non-Null Count   Dtype         \n",
      "---  ------                   --------------   -----         \n",
      " 0   User_id                  355210 non-null  int64         \n",
      " 1   Product_id               355210 non-null  int64         \n",
      " 2   Rating                   355210 non-null  int64         \n",
      " 3   Date                     355210 non-null  datetime64[ns]\n",
      " 4   Review                   355210 non-null  object        \n",
      " 5   Spam(1) and Not Spam(0)  355210 non-null  int64         \n",
      " 6   Sentiment                355210 non-null  object        \n",
      " 7   Features                 355210 non-null  object        \n",
      "dtypes: datetime64[ns](1), int64(4), object(3)\n",
      "memory usage: 21.7+ MB\n"
     ]
    }
   ],
   "source": [
    "yelp_pd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_reviews_pd = yelp_pd[[\"Review\",\"Spam(1) and Not Spam(0)\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespace(text):\n",
    "    \"\"\"Remove Whitespace\"\"\"\n",
    "    if isinstance(text,str):\n",
    "        return \" \".join(text.split())\n",
    "    else:\n",
    "        return \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yelp_reviews_pd.head()['Review'].apply(remove_whitespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-14ab0e4039fe>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  yelp_reviews_pd['Review'] = yelp_reviews_pd['Review'].apply(remove_whitespace)\n",
      "<ipython-input-9-14ab0e4039fe>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  yelp_reviews_pd['Review'] = yelp_reviews_pd['Review'].str.lower()\n"
     ]
    }
   ],
   "source": [
    "yelp_reviews_pd['Review'] = yelp_reviews_pd['Review'].apply(remove_whitespace)\n",
    "yelp_reviews_pd['Review'] = yelp_reviews_pd['Review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    the food at snack is a selection of popular gr...\n",
       "1    this little place in soho is wonderful. i had ...\n",
       "2    ordered lunch for 15 from snack last friday. ã...\n",
       "3    this is a beautiful quaint little restaurant o...\n",
       "4    snack is great place for a ã‚â casual sit down...\n",
       "Name: Review, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_reviews_pd['Review'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =yelp_reviews_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['review', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide `cuda` or `cpu`\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-b5d5653f7743>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['data_type'] = ['not_set']*df.shape[0]\n",
      "C:\\opt\\software\\Anacoda3\\lib\\site-packages\\pandas\\core\\indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th>data_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>train</th>\n",
       "      <td>271215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>47862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>train</th>\n",
       "      <td>30713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>5420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 review\n",
       "label data_type        \n",
       "0     train      271215\n",
       "      val         47862\n",
       "1     train       30713\n",
       "      val          5420"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split Trainging, Validation Dataset, with 15% splitting.\n",
    "X_train, X_val, y_train, y_val = train_test_split(df.index.values, \n",
    "                                                  df.label.values, \n",
    "                                                  test_size=0.15, \n",
    "                                                  random_state=42, \n",
    "                                                  stratify=df.label.values)\n",
    "\n",
    "df['data_type'] = ['not_set']*df.shape[0]\n",
    "\n",
    "df.loc[X_train, 'data_type'] = 'train'\n",
    "df.loc[X_val, 'data_type'] = 'val'\n",
    "\n",
    "df.groupby(['label', 'data_type']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case=True)\n",
    "                                          \n",
    "# Wrap training dataset\n",
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    df[df.data_type=='train'].review.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=256, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Wrap validation dataset\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    df[df.data_type=='val'].review.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=256, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(df[df.data_type=='train'].label.values)\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(df[df.data_type=='val'].label.values)\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ae18dd5a4a84387ad054887cf59a043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b48f1ab81243bbac17412123bed091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Generate BERT model.\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",  # Scientific Specific Tokenizer\n",
    "                                                      num_labels=2,\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Size\n",
    "batch_size = 3\n",
    "\n",
    "# Training DataLoader\n",
    "dataloader_train = DataLoader(dataset_train, \n",
    "                              sampler=RandomSampler(dataset_train), \n",
    "                              batch_size=batch_size)\n",
    "\n",
    "# Validation DataLoader\n",
    "dataloader_validation = DataLoader(dataset_val, \n",
    "                                   sampler=SequentialSampler(dataset_val), \n",
    "                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdamW as Optimizer\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=1e-5, \n",
    "                  eps=1e-8)\n",
    "\n",
    "# Epochs\n",
    "epochs = 8# Better change to 5\n",
    "\n",
    "# Warmup Algorithm for Optimization.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_func(preds, labels):\n",
    "    \"\"\"F1-Score Metrics\"\"\"\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "def accuracy_per_class(preds, labels):\n",
    "    \"\"\"Accuracy for Classes\"\"\"\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    \n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Random Seeds.\n",
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "def evaluate(dataloader_val):\n",
    "    \"\"\"\n",
    "    Evaluation for Val Dataset\n",
    "    Return: Loss for Val, Prediction, Grand Truth\n",
    "    \"\"\"\n",
    "    # Close Gradient Desc\n",
    "    model.eval()\n",
    "    \n",
    "    # Calculate the Loss for Val.\n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    # Traverse the Val Data.\n",
    "    for batch in dataloader_val:\n",
    "        \n",
    "        batch = tuple(b.cuda() for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        # Does not calculate the Gradient.\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "    \n",
    "    # \n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e60c6d0aa64c4395bbd2b57705606e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 1', max=100643.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training loss: 0.45929818270800143\n",
      "Validation loss: 0.47324350842297114\n",
      "F1 Score (Weighted): 0.8501411485205623\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 2', max=100643.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Training loss: 0.4621436762859857\n",
      "Validation loss: 0.47602949271269174\n",
      "F1 Score (Weighted): 0.8501411485205623\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 3', max=100643.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3\n",
      "Training loss: 0.45832859668113846\n",
      "Validation loss: 0.4100581753007318\n",
      "F1 Score (Weighted): 0.8501411485205623\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 4', max=100643.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4\n",
      "Training loss: 0.45250498087408386\n",
      "Validation loss: 0.45503736005672424\n",
      "F1 Score (Weighted): 0.8501411485205623\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 5', max=100643.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5\n",
      "Training loss: 0.44910261330340695\n",
      "Validation loss: 0.408989434822283\n",
      "F1 Score (Weighted): 0.8501411485205623\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6dab9421bdc44d3aebbbbb8cae50b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 6', max=100643.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the Model.\n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    # Open for Gradient Calculation.\n",
    "    model.train()\n",
    "    \n",
    "    # Loss for Train\n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "\n",
    "        # Clear the Current Gradient\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }       \n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        # Adding the Loss\n",
    "        loss_train_total += loss.item()\n",
    "        \n",
    "        # Back Propagation\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Step for Optimizer\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "        \n",
    "    torch.save(model.state_dict(), f'finetuned_BERT_epoch_{epoch}.model')\n",
    "        \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    # Evaluation for Val Dataset.\n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (Weighted): {val_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Test Dataset.\n",
    "test_df = pd.read_csv('test_data.csv')\n",
    "test_df['abstract'] = test_df['abstract'].apply(remove_whitespace)\n",
    "test_df['abstract'] = test_df['abstract'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.columns = ['train_id', 'Title']\n",
    "test_df['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap up Test Data.\n",
    "test_data = tokenizer.batch_encode_plus(\n",
    "    test_df.Title.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=256, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "input_ids_test = test_data['input_ids']\n",
    "attention_masks_test = test_data['attention_mask']\n",
    "labels_test = torch.tensor(test_df.label.values)\n",
    "dataset_test = TensorDataset(input_ids_test, attention_masks_test, labels_test)\n",
    "\n",
    "# Test DataLoader\n",
    "dataloader_test = DataLoader(dataset_test, \n",
    "                              sampler=SequentialSampler(dataset_test), \n",
    "                              batch_size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('finetuned_BERT_epoch_2.model', map_location=torch.device('cuda')))\n",
    "\n",
    "# Generate Prediction\n",
    "_, predictions, true_vals = evaluate(dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inversely Transfer to ClassNames.\n",
    "label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "preds_flat = np.argmax(predictions, axis=1).flatten()\n",
    "test_df['predict']= preds_flat\n",
    "test_df['predict'] = test_df['predict'].replace(label_dict_inverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.drop(['Title','label'], axis=1, inplace=True)\n",
    "test_df.columns = ['test_id', 'label']\n",
    "test_df.to_csv('pred_labels.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] BERT Text Classification Using Pytorch. https://towardsdatascience.com/bert-text-classification-using-pytorch-723dfb8b6b5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
